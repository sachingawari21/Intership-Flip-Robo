{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5942f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)\tWrite a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the header tags (h1 to h6) using the find_all method\n",
    "header_tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "\n",
    "# Extract the text from the header tags and store them in a list\n",
    "header_texts = [tag.get_text() for tag in header_tags]\n",
    "\n",
    "# Create a data frame using pandas\n",
    "df = pd.DataFrame(header_texts, columns=[\"Header\"])\n",
    "\n",
    "# Display the data frame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618cfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2)\tWrite s python program to display list of respected former presidents of India\n",
    "#(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents.htm and make data frame.\n",
    "\n",
    "#https://www.icc-cricket.com/rankings/team-rankings/mens/odi\n",
    "\n",
    "#pip install requests beautifulsoup4 pandas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url =\"https://www.icc-cricket.com/rankings/team-rankings/mens/odi\"\n",
    "\n",
    "# Fetch the HTML content from the URL\n",
    "response = requests.get(url)\n",
    "html_content = response.content\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the table containing the information about former presidents\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extract the data from the table\n",
    "data = []\n",
    "for row in table.find_all('tr')[1:]: \n",
    "    columns = row.find_all('tr')\n",
    "    name = columns[0].text.strip()\n",
    "    term_of_office = columns[1].text.strip()\n",
    "    data.append([name, term_of_office])\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [name,term_of_office]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3)\tWrite a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "#a)\tTop 10 ODI teams in menâ€™s cricket along with the records for matches, points and rating.\n",
    "#b)\tTop 10 ODI Batsmen along with the records of their team andrating.\n",
    "#c)\tTop 10 ODI bowlers along with the records of their team andrating.\n",
    "\n",
    "#https://www.icc-cricket.com/rankings/team-rankings/mens/odi\n",
    "#To scrape cricket rankings from icc-cricket.com and create a data frame, you can use the BeautifulSoup library in Python. Here's how you can do it:\n",
    "#a) To scrape the top 10 ODI teams in men's cricket along with the records for matches, points, and rating, you can use the following code:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    team = cells[1].text.strip()\n",
    "    matches = cells[2].text.strip()\n",
    "    points = cells[3].text.strip()\n",
    "    rating = cells[4].text.strip()\n",
    "    team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)\n",
    "#b) To scrape the top 10 ODI batsmen along with the records of their team and rating, you can use the following code:\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    batsman = cells[1].text.strip()\n",
    "    team = cells[2].text.strip()\n",
    "    rating = cells[3].text.strip()\n",
    "    batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)\n",
    "#c) To scrape the top 10 ODI bowlers along with the records of their team and rating, you can use the following code:\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "    cells = row.find_all(\"td\")\n",
    "    bowler = cells[1].text.strip()\n",
    "    team = cells[2].text.strip()\n",
    "    rating = cells[3].text.strip()\n",
    "    bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0490f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_odi_teams_data(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        teams_data = []\n",
    "        for row in soup.find_all('tr', class_='rankings-block__banner') + soup.find_all('tr', class_='table-body'):\n",
    "            columns = row.find_all(['th', 'td'])\n",
    "            team_name = columns[1].text.strip()\n",
    "            matches = columns[2].text.strip()\n",
    "            points = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            \n",
    "            teams_data.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "            \n",
    "            if len(teams_data) == 10:\n",
    "                break\n",
    "        \n",
    "        return teams_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_odi_players_data(url, player_type):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        players_data = []\n",
    "        for row in soup.find_all('tr', class_='rankings-block__banner') + soup.find_all('tr', class_='table-body'):\n",
    "            columns = row.find_all(['th', 'td'])\n",
    "            player_name = columns[1].text.strip()\n",
    "            team_name = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            players_data.append({'Player': player_name, 'Team': team_name, 'Rating': rating})\n",
    "            \n",
    "            if len(players_data) == 10:\n",
    "                break\n",
    "        \n",
    "        return players_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Top 10 ODI Teams\n",
    "    odi_teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    top_odi_teams_data = get_odi_teams_data(odi_teams_url)\n",
    "    \n",
    "    if top_odi_teams_data:\n",
    "        df_teams = pd.DataFrame(top_odi_teams_data)\n",
    "        print(\"Top 10 ODI Teams:\")\n",
    "        print(df_teams)\n",
    "    else:\n",
    "        print(\"Failed to fetch ODI teams data.\")\n",
    "    \n",
    "    # Top 10 ODI Batsmen\n",
    "    odi_batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "    top_odi_batsmen_data = get_odi_players_data(odi_batsmen_url, \"Batsmen\")\n",
    "    \n",
    "    if top_odi_batsmen_data:\n",
    "        df_batsmen = pd.DataFrame(top_odi_batsmen_data)\n",
    "        print(\"\\nTop 10 ODI Batsmen:\")\n",
    "        print(df_batsmen)\n",
    "    else:\n",
    "        print(\"Failed to fetch ODI batsmen data.\")\n",
    "    \n",
    "    # Top 10 ODI Bowlers\n",
    "    odi_bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "    top_odi_bowlers_data = get_odi_players_data(odi_bowlers_url, \"Bowlers\")\n",
    "    \n",
    "    if top_odi_bowlers_data:\n",
    "        df_bowlers = pd.DataFrame(top_odi_bowlers_data)\n",
    "        print(\"\\nTop 10 ODI Bowlers:\")\n",
    "        print(df_bowlers)\n",
    "    else:\n",
    "        print(\"Failed to fetch ODI bowlers data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d8f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4)\tWrite a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "#a)\tTop 10 ODI teams in womenâ€™s cricket along with the records for matches, points and rating.\n",
    "#b)\tTop 10 womenâ€™s ODI Batting players along with the records of their team and rating.\n",
    "#c)\tTop 10 womenâ€™s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "\n",
    "\n",
    "#To scrape cricket rankings from icc-cricket.com and create a data frame, you can use the BeautifulSoup library in Python. Here's an example code that demonstrates how to scrape the required information:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_odi_teams(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    teams_data = []\n",
    "    table = soup.find('table', class_='table')\n",
    "    for row in table.find_all('tr')[1:11]:  # Top 10 teams\n",
    "        columns = row.find_all('td')\n",
    "        team_name = columns[1].text.strip()\n",
    "        matches = columns[2].text.strip()\n",
    "        points = columns[3].text.strip()\n",
    "        rating = columns[4].text.strip()\n",
    "        teams_data.append([team_name, matches, points, rating])\n",
    "\n",
    "    columns = ['Team', 'Matches', 'Points', 'Rating']\n",
    "    df = pd.DataFrame(teams_data, columns=columns)\n",
    "    return df\n",
    "\n",
    "def scrape_odi_batting_players(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    players_data = []\n",
    "    table = soup.find('table', class_='table rankings-table')\n",
    "    for row in table.find_all('tr')[1:11]:  # Top 10 players\n",
    "        columns = row.find_all('td')\n",
    "        player_name = columns[1].text.strip()\n",
    "        team_name = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        players_data.append([player_name, team_name, rating])\n",
    "\n",
    "    columns = ['Player', 'Team', 'Rating']\n",
    "    df = pd.DataFrame(players_data, columns=columns)\n",
    "    return df\n",
    "\n",
    "def scrape_odi_all_rounders(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    all_rounders_data = []\n",
    "    table = soup.find('table', class_='table rankings-table')\n",
    "    for row in table.find_all('tr')[1:11]:  # Top 10 all-rounders\n",
    "        columns = row.find_all('td')\n",
    "        player_name = columns[1].text.strip()\n",
    "        team_name = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        all_rounders_data.append([player_name, team_name, rating])\n",
    "\n",
    "    columns = ['Player', 'Team', 'Rating']\n",
    "    df = pd.DataFrame(all_rounders_data, columns=columns)\n",
    "    return df\n",
    "\n",
    "# URLs for the respective rankings\n",
    "odi_teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "odi_batting_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "odi_all_rounders_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "\n",
    "# Scrape and display the data frames\n",
    "print(\"Top 10 ODI Teams in Women's Cricket:\")\n",
    "df_teams = scrape_odi_teams(odi_teams_url)\n",
    "print(df_teams)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "df_batting = scrape_odi_batting_players(odi_batting_url)\n",
    "print(df_batting)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI All-Rounders:\")\n",
    "df_all_rounders = scrape_odi_all_rounders(odi_all_rounders_url)\n",
    "print(df_all_rounders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_womens_odi_teams_data(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        teams_data = []\n",
    "        for row in soup.find_all('tr', class_='rankings-block__banner') + soup.find_all('tr', class_='table-body'):\n",
    "            columns = row.find_all(['th', 'td'])\n",
    "            team_name = columns[1].text.strip()\n",
    "            matches = columns[2].text.strip()\n",
    "            points = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            \n",
    "            teams_data.append({'Team': team_name, 'Matches': matches, 'Points': points, 'Rating': rating})\n",
    "            \n",
    "            if len(teams_data) == 10:\n",
    "                break\n",
    "        \n",
    "        return teams_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def get_womens_odi_players_data(url, player_type):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        players_data = []\n",
    "        for row in soup.find_all('tr', class_='rankings-block__banner') + soup.find_all('tr', class_='table-body'):\n",
    "            columns = row.find_all(['th', 'td'])\n",
    "            player_name = columns[1].text.strip()\n",
    "            team_name = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            players_data.append({'Player': player_name, 'Team': team_name, 'Rating': rating})\n",
    "            \n",
    "            if len(players_data) == 10:\n",
    "                break\n",
    "        \n",
    "        return players_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Top 10 Women's ODI Teams\n",
    "    womens_odi_teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "    top_womens_odi_teams_data = get_womens_odi_teams_data(womens_odi_teams_url)\n",
    "    \n",
    "    if top_womens_odi_teams_data:\n",
    "        df_womens_teams = pd.DataFrame(top_womens_odi_teams_data)\n",
    "        print(\"Top 10 Women's ODI Teams:\")\n",
    "        print(df_womens_teams)\n",
    "    else:\n",
    "        print(\"Failed to fetch Women's ODI teams data.\")\n",
    "    \n",
    "    # Top 10 Women's ODI Batsmen\n",
    "    womens_odi_batsmen_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "    top_womens_odi_batsmen_data = get_womens_odi_players_data(womens_odi_batsmen_url, \"Batsmen\")\n",
    "    \n",
    "    if top_womens_odi_batsmen_data:\n",
    "        df_womens_batsmen = pd.DataFrame(top_womens_odi_batsmen_data)\n",
    "        print(\"\\nTop 10 Women's ODI Batsmen:\")\n",
    "        print(df_womens_batsmen)\n",
    "    else:\n",
    "        print(\"Failed to fetch Women's ODI batsmen data.\")\n",
    "    \n",
    "    # Top 10 Women's ODI All-rounders\n",
    "    womens_odi_allrounders_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "    top_womens_odi_allrounders_data = get_womens_odi_players_data(womens_odi_allrounders_url, \"All-rounders\")\n",
    "    \n",
    "    if top_womens_odi_allrounders_data:\n",
    "        df_womens_allrounders = pd.DataFrame(top_womens_odi_allrounders_data)\n",
    "        print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "        print(df_womens_allrounders)\n",
    "    else:\n",
    "        print(\"Failed to fetch Women's ODI all-rounders data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7148628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5)\tWrite a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "#i)\tHeadline\n",
    "#ii)\tTime\n",
    "#iii)\tNews Link\n",
    "\n",
    "#To scrape news details from a website and create a dataframe in Python, you can use the BeautifulSoup library along with the requests library. Here's an example code that scrapes the headline, time, and news link from the given CNBC website:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the news articles on the page\n",
    "articles = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "# Loop through each article and extract the required information\n",
    "for article in articles:\n",
    "  # Extract the headline\n",
    "    headline = article.find(\"a\").text.strip()\n",
    "    headlines.append(headline)\n",
    "  \n",
    "  # Extract the time\n",
    "    time = article.find(\"time\").text.strip()\n",
    "    times.append(time)\n",
    "  \n",
    "  # Extract the news link\n",
    "    link = article.find(\"a\")[\"href\"]\n",
    "    links.append(link)\n",
    "\n",
    "# Create a dataframe using the scraped data\n",
    "data = {\n",
    "  \"Headline\": headlines,\n",
    "  \"Time\": times,\n",
    "  \"News Link\": links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6)\tWrite a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "#i)\tPaper Title\n",
    "#ii)\tAuthors\n",
    "#iii)\tPublished Date\n",
    "#iv)\tPaper URL\n",
    "\n",
    "\n",
    "\n",
    "#To scrape the details of the most downloaded articles from the given website and create a dataframe with the following information: Paper Title, Authors, Published Date, and Paper URL, you can use the BeautifulSoup library in Python. Here's an example code to achieve this:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container that holds the article details\n",
    "articles_container = soup.find(\"div\", class_=\"pod-listing\")\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "# Iterate over each article in the container\n",
    "for article in articles_container.find_all(\"li\"):\n",
    "  # Scrape the title\n",
    "    title = article.find(\"h3\").text.strip()\n",
    "    titles.append(title)\n",
    "  \n",
    "  # Scrape the authors\n",
    "    author = article.find(\"span\", class_=\"text-xs\").text.strip()\n",
    "    authors.append(author)\n",
    "  \n",
    "  # Scrape the published date\n",
    "    date = article.find(\"span\", class_=\"text-xs\").find_next_sibling(\"span\").text.strip()\n",
    "    dates.append(date)\n",
    "  \n",
    "  # Scrape the paper URL\n",
    "    url = article.find(\"a\")[\"href\"]\n",
    "    urls.append(url)\n",
    "\n",
    "# Create a dataframe with the scraped data\n",
    "data = {\n",
    "  \"Paper Title\": titles,\n",
    "  \"Authors\": authors,\n",
    "  \"Published Date\": dates,\n",
    "  \"Paper URL\": urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3beb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        articles_data = []\n",
    "        for article in soup.find_all('div', class_='js-article-content'):\n",
    "            title = article.find('a', class_='article-title').text.strip()\n",
    "            authors = article.find('div', class_='Authors').text.strip()\n",
    "            published_date = article.find('div', class_='js-article-details').find('div', class_='article-details').text.strip()\n",
    "            paper_url = \"https://www.journals.elsevier.com\" + article.find('a', class_='article-title')['href']\n",
    "            \n",
    "            articles_data.append({\n",
    "                'Paper Title': title,\n",
    "                'Authors': authors,\n",
    "                'Published Date': published_date,\n",
    "                'Paper URL': paper_url\n",
    "            })\n",
    "        \n",
    "        return articles_data\n",
    "    else:\n",
    "        print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    elsevier_url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "    most_downloaded_articles_data = get_most_downloaded_articles(elsevier_url)\n",
    "    \n",
    "    if most_downloaded_articles_data:\n",
    "        df_articles = pd.DataFrame(most_downloaded_articles_data)\n",
    "        print(\"Most Downloaded Articles DataFrame:\")\n",
    "        print(df_articles)\n",
    "    else:\n",
    "        print(\"Failed to fetch most downloaded articles data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c650a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#7)\tWrite a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "#i)\tRestaurant name\n",
    "#ii)\tCuisine\n",
    "#iii)\tLocation\n",
    "#iv)\tRatings\n",
    "#v)\tImage URL\n",
    "\n",
    "\n",
    "#To scrape the mentioned details from the dineout.co.in website and create a dataframe in Python, you can use the BeautifulSoup library for web scraping and the pandas library for creating the dataframe. Here's an example code snippet to get you started:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url =\"https://www.dineout.co.in\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the elements containing the details you want to scrape\n",
    "restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines = soup.find_all('span', class_='double-line-ellipsis')\n",
    "locations = soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('span', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')\n",
    "\n",
    "# Create empty lists to store the scraped data\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []\n",
    "\n",
    "# Extract the data from the elements and append them to the respective lists\n",
    "for name in restaurant_names:\n",
    "    restaurant_list.append(name.text.strip())\n",
    "\n",
    "for cuisine in cuisines:\n",
    "    cuisine_list.append(cuisine.text.strip())\n",
    "\n",
    "for location in locations:\n",
    "    location_list.append(location.text.strip())\n",
    "\n",
    "for rating in ratings:\n",
    "    rating_list.append(rating.text.strip())\n",
    "\n",
    "for image in image_urls:\n",
    "    image_url_list.append(image['src'])\n",
    "\n",
    "# Create a dictionary from the lists\n",
    "data = {\n",
    "  'Restaurant Name': restaurant_list,\n",
    "  'Cuisine': cuisine_list,\n",
    "  'Location': location_list,\n",
    "  'Ratings': rating_list,\n",
    "  'Image URL': image_url_list\n",
    "}\n",
    "\n",
    "# Create a dataframe from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
