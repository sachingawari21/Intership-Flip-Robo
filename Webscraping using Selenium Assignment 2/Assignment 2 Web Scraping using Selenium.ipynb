{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12db97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. \n",
    "#You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "#This task will be done in following steps:\n",
    "#1.\tFirst get the webpage https://www.shine.com/\n",
    "#2.\tEnter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "#3.\tThen click the searchbutton.\n",
    "#4.\tThen scrape the data for the first 10 jobs results you get.\n",
    "#5.\tFinally create a dataframe of the scraped data.\n",
    "#Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acfa174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cda420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening naukri.com\n",
    "url ='https://www.shine.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba70f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"qsb-keyword-sugg\"]').send_keys('Data Analyst')\n",
    "driver.find_element_by_xpath('//*[@id=\"qsb-location-sugg\"]').send_keys('Bangalore')\n",
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section/div/form/div[3]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store job title, job location, company name, experiecne require\n",
    "job_title =[]\n",
    "job_location=[]\n",
    "company =[]\n",
    "experienced=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81033182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping job title, job location, company name, experiecne require from shine website\n",
    "titles=driver.find_elements_by_xpath('//div[@class=\"info fleft\"]/a')[:10]\n",
    "company_title=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')[:10]\n",
    "company_location= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')[:10]\n",
    "experience_require=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ff574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending scrap data in lists\n",
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "for i in company_location:\n",
    "    job_location.append(i.text)\n",
    "for i in experience_require:\n",
    "    experience=i.text\n",
    "    experienced.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de241283",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_analyst=pd.DataFrame({})\n",
    "Data_analyst['JOB TITLE']=job_title\n",
    "Data_analyst['COMPANY']=company\n",
    "Data_analyst['JOB LOCATION']=job_location\n",
    "Data_analyst['Experience Require']=experienced\n",
    "print('/033[1m'+'Top 10 Data Analyst Job at Bangrole :'+'/033[0m')\n",
    "Data_analyst[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486d255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location.\n",
    "#You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "#This task will be done in following steps:\n",
    "#1.\tFirst get the webpage https://www.shine.com/\n",
    "#2.\tEnter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "#3.\tThen click the search button.\n",
    "#4.\tThen scrape the data for the first 10 jobs results you get.\n",
    "#5.\tFinally create a dataframe of the scraped data.#\n",
    "#Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ab107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening shine.com\n",
    "url ='https://www.shine.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d761ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"qsb-keyword-sugg\"]').send_keys('Data Scientist')\n",
    "driver.find_element_by_xpath('//*[@id=\"qsb-location-sugg\"]').send_keys('Bangalore')\n",
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section/div/form/div[3]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty lists\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the list of url of first 10 data scientist jobs\n",
    "\n",
    "URL=[]\n",
    "job_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")[0:10]\n",
    "for i in job_tags:\n",
    "    URL.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in URL:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        job_title.append((driver.find_element_by_xpath('//*[@id=\"root\"]/main/div[2]/div[2]/section[1]/div[1]/div[1]/header/h1')).text)\n",
    "        job_location.append((driver.find_element_by_xpath('//*[@id=\"root\"]/main/div[2]/div[2]/section[1]/div[1]/div[2]/div[3]/span')).text)\n",
    "        company_name.append((driver.find_element_by_xpath('//*[@id=\"root\"]/main/div[2]/div[2]/section[1]/div[1]/div[1]/div/a')).text)\n",
    "        job_description.append((driver.find_element_by_xpath('//*[@id=\"root\"]/main/div[2]/div[2]/section[2]')).text.replace(\"\\n\",\"\"))\n",
    "    except NoSuchElementException:\n",
    "        job_title.append(\"Nan\")\n",
    "        job_location.append(\"Nan\")\n",
    "        company_name.append(\"Nan\")\n",
    "        job_description.append(\"Nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scientist=pd.DataFrame({})\n",
    "Data_scientist['job_title']=job_title\n",
    "Data_scientist['job_location']=job_location\n",
    "Data_scientist['company_name']=company_name\n",
    "Data_scientist['job_description']=job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b3510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m'+'Top 10 Data Analyst Job at Bangrole :'+'\\033[0m')\n",
    "Data_scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3: In this question you have to scrape data using the filters available on the webpage\n",
    "#You have to use the location and salary filter.\n",
    "#You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "#You have to scrape the job-title, job-location, company name, experience required. \n",
    "#The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "#The task will be done as shown in the below steps:\n",
    "#1.\tfirst get the web page https://www.shine.com/\n",
    "#2.\tEnter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "#3.\tThen click the search button.\n",
    "#4.\tThen apply the location filter and salary filter by checking the respective boxes\n",
    "#5.\tThen scrape the data for the first 10 jobs results you get.\n",
    "#6.\tFinally create a dataframe of the scraped data.\n",
    "#Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening shine.com\n",
    "url ='https://www.shine.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1535b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"qsb-keyword-sugg\"]').send_keys('Data Scientist')\n",
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section/div/form/div[3]/button').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968acb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticking on Delhi/NCR \n",
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[3]/label/p/span[1]').click()\n",
    "\n",
    "# Ticking on 3-6 lakhs Salary\n",
    "driver.find_element_by_xpath('//*[@id=\"root\"]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/p/span[1]').click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14989865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists to store job title, job location, company name, experiecne require\n",
    "job_title =[]\n",
    "job_location=[]\n",
    "company =[]\n",
    "experienced=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')[:10]\n",
    "company_title=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')[:10]\n",
    "company_location= driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')[:10]\n",
    "experience_require=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")[0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "for i in company_title:\n",
    "    company.append(i.text)\n",
    "for i in company_location:\n",
    "    job_location.append(i.text)\n",
    "for i in experience_require:\n",
    "    experience=i.text\n",
    "    experienced.append(experience)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scientist=pd.DataFrame({})\n",
    "Data_scientist['JOB TITLE']=job_title\n",
    "Data_scientist['COMPANY']=company\n",
    "Data_scientist['JOB LOCATION']=job_location\n",
    "Data_scientist['Experience Require']=experienced\n",
    "print('/033[1m'+'Top 10 Data Analyst Job at Bangrole :'+'/033[0m')\n",
    "Data_scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "#6.\tBrand\n",
    "#7.\tProduct Description\n",
    "#8.\tPrice\n",
    "#The attributes which you have to scrape is ticked marked in the below image.\n",
    "#To scrape the data you have to go through following steps:\n",
    "#1.\tGo to Flipkart webpage by url : https://www.flipkartcom/\n",
    "#2.\tEnter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "#3.\tAfter that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "#4.\tAfter scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "#click on it.\n",
    "#5.\tNow scrape data from this page as usual\n",
    "#6.\tRepeat this until you get data for 100sunglasses.\n",
    "#Note: That all of the above steps have to be done by coding only and not manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce77483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening www.flipkartcom\n",
    "url ='https://www.flipkartcom'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96306d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_In = driver.find_element_by_xpath('//button[@class=\"d-flex align-items-center justify-content-center order-1 order-md-2 mr-auto mr-md-0 p-0 LockedHomeHeaderStyles__signInButton\"]')\n",
    "sign_In.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E_id=driver.find_element_by_id(\"userEmail\").send_keys('sachin.hari4@gmail.com')\n",
    "P_id=driver.find_element_by_id(\"userPassword\").send_keys('Welcome@123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Click1=driver.find_element_by_xpath('//button[@class=\"gd-ui-button minWidthBtn css-8i7bc2\"]')\n",
    "Click1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_id(\"sc.keyword\").send_keys(\"sunglasses\")\n",
    "driver.find_element_by_xpath('//*[@id=\"sc.location\"]').send_keys(\"\n",
    "search for products, brands and more\")\n",
    "driver.find_element_by_xpath('//*[@id=\"scBar\"]/div/button/span').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "company=[]\n",
    "date =[]\n",
    "URL=[]\n",
    "company_rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "company_name=driver.find_elements_by_xpath('//div[@class=\"d-flex justify-content-between align-items-start\"]/a/span')\n",
    "time.sleep(5)\n",
    "date_post=driver.find_elements_by_xpath('//div[@class=\"d-flex align-items-end pl-std css-17n8uzw\"]')\n",
    "time.sleep(5)\n",
    "url_extract=driver.find_elements_by_xpath('//a[@class=\"jobLink job-search-key-1rd3saf eigr9kq1\"]')\n",
    "time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ebf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in company_name:\n",
    "    a=i.text\n",
    "    company.append(a)\n",
    "    time.sleep(2)\n",
    "for i in date_post:\n",
    "    b=i.text\n",
    "    date.append(b)\n",
    "    time.sleep(2)\n",
    "for i in url_extract:\n",
    "    URL.append(i.get_attribute('href'))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a11406",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in URL[:10]:\n",
    "    driver.get(i)\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        company_rating.append((driver.find_element_by_xpath('//span[@class=\"css-1pmc6te e11nt52q4\"]')).text[:3])\n",
    "    except NoSuchElementException:\n",
    "        company_rating.append('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3fdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_scientist=pd.DataFrame({})\n",
    "Data_scientist['COMPANY']=company[:10]\n",
    "Data_scientist['Posted Ago']=date[:10]\n",
    "Data_scientist['Rating']=company_rating[:10]\n",
    "print('\\033[1m'+'search for products, brands and more :'+'\\033[0m')\n",
    "Data_scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafbeb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. \n",
    "#You have to go the link: \n",
    "#https://www.flipkart.com/apple-iphone-11-black-64-gb/product- reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market place=FLIPKART\n",
    "\n",
    "#As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "#1.\tRating\n",
    "#2.\tReview summary\n",
    "#3.\tFull review\n",
    "#4.\tYou have to scrape this data for first 100reviews.\n",
    "#Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9971937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e84f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//div[@class=\"_3UAT2v _16PBlm\"]/span').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3fe7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "#creating empty lists\n",
    "Rating=[]\n",
    "Review_summary=[]\n",
    "Full_review=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7ec2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,12):\n",
    "        \n",
    "    rat = driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    rev = driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]')\n",
    "    frev = driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]')\n",
    "\n",
    "    for k in rat:\n",
    "        rating.append(k.text)\n",
    "\n",
    "    for i in rev:\n",
    "        review_summary.append(i.text)\n",
    "\n",
    "    for j in frev:\n",
    "        full_review.append(j.text)\n",
    "    time.sleep(3)\n",
    "\n",
    "    print(len(review_summary))\n",
    "    \n",
    "    nxt_page = driver.find_elements_by_xpath('//a[@class=\"_1LKTO3\"]')\n",
    "    try:\n",
    "        driver.get(nxt_page[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_page[0].get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b039515",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m'+'Iphone 11 Reviews from Flipkart :'+'\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a77754",
   "metadata": {},
   "outputs": [],
   "source": [
    "Review_Iphone11= pd.DataFrame({})\n",
    "Review_Iphone11['Rating'] = rating[:100]\n",
    "Review_Iphone11['Review Summary'] = review_summary[:100] \n",
    "Review_Iphone11['Full Review'] = full_review[:100]\n",
    "Review_Iphone11.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6.Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "#You have to scrape 3 attributes of each sneaker:\n",
    "#1.\tBrand\n",
    "#2.\tProduct Description\n",
    "#3.\tPrice\n",
    "#As shown in the below image, you have to scrape the above attributes.\n",
    "# let's first connect to the web driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e261410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//div[@class=\"_3OO5Xc\"]/input').send_keys('sneakers')\n",
    "driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f1335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the empty lists\n",
    "Brand=[]\n",
    "Product_Description=[]\n",
    "Price=[]\n",
    "Discount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890aeb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the list of url of first 100 sneakers\n",
    "URL=[]\n",
    "for i in range(0,4):\n",
    "    sneakers=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    for i in sneakers:\n",
    "        URL.append(i.get_attribute('href'))\n",
    "    driver.find_element_by_xpath('//a[@class=\"ge-49M\"]').click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9206bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brand),len(Product_Description),len(Price),len(Discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65eb46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sneakers=pd.DataFrame({})\n",
    "Sneakers['Brand']=Brand[:100]\n",
    "Sneakers['Product_Description']=Product_Description[:100]\n",
    "Sneakers['Price']=Price[:100]\n",
    "Sneakers['Discount']=Discount[:100]\n",
    "Sneakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e93076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "#set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "#After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "#1.\tTitle\n",
    "#2.\tRatings\n",
    "#3.\tPrice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7eed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# connecting Webdriver\n",
    "driver = webdriver.Chrome(r'C:\\chromedriver.exe')\n",
    "driver\n",
    "\n",
    "# Opening naukri.com\n",
    "url ='https://www.amazon.in/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a09a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"twotabsearchtextbox\"]').send_keys('laptop')\n",
    "driver.find_element_by_xpath('//*[@id=\"nav-search-submit-button\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Laptop_Model=[]\n",
    "Ratings=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26aa40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the list of url of laptops\n",
    "URL=[]\n",
    "\n",
    "laptop_url=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "for i in laptop_url:\n",
    "    laptop=i.find_element_by_tag_name(\"a\")\n",
    "    URL.append(laptop.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13682992",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slicing to get url of first 10 laptop\n",
    "list_of_url=URL[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d358e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(list_of_url):\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        model=driver.find_element_by_xpath('//*[@id=\"productTitle\"]').text\n",
    "        Laptop_Model.append(model)\n",
    "    except NoSuchElementException:\n",
    "        Laptop_Model.append('NaN')\n",
    "        \n",
    "    try:\n",
    "        price=driver.find_element_by_xpath('//*[@id=\"priceblock_dealprice\"]').text\n",
    "        Price.append(price)\n",
    "    except NoSuchElementException:\n",
    "        price1=driver.find_element_by_xpath('//*[@id=\"priceblock_ourprice\"]').text\n",
    "        Price.append(price1)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        ratings=driver.find_element_by_xpath('//span[@class=\"a-size-base a-nowrap\"]/span').text\n",
    "        Ratings.append(ratings)\n",
    "    except NoSuchElementException:\n",
    "        Ratings.append('NaN')\n",
    "        \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f437cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Laptops=pd.DataFrame({})\n",
    "Laptops['Laptop Models']=Laptop_Model\n",
    "Laptops['Price']=Price\n",
    "Laptops['Ratings']=Ratings\n",
    "print('\\033[1m'+'Top Laptop Model on Amazon'+'\\033[0m')\n",
    "Laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208d6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "#The above task will be done in following steps:\n",
    "#1.\tFirst get the webpagehttps://www.azquotes.com/\n",
    "#2.\tClick on Top Quotes\n",
    "#3.\tThan scrap a) Quote b) Author c) Type Of Quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d39e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.azquotes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96169eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_quotes_button = driver.find_element(By.LINK_TEXT, \"Top Quotes\")\n",
    "top_quotes_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554baba",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = driver.find_elements(By.CSS_SELECTOR, \".title a\")\n",
    "authors = driver.find_elements(By.CSS_SELECTOR, \".author a\")\n",
    "types = driver.find_elements(By.CSS_SELECTOR, \".kw-box a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940eface",
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote, author, quote_type in zip(quotes, authors, types):\n",
    "print(\"Quote:\", quote.text)\n",
    "print(\"Author:\", author.text)\n",
    "print(\"Type of Quote:\", quote_type.text)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9: Write a python program to display list of respected former Prime Ministers of \n",
    "    #India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "\n",
    "#This task will be done in following steps:\n",
    "#1.\tFirst get the webpagehttps://www.jagranjosh.com/\n",
    "#2.\tThen You have to click on the GK option\n",
    "#3.\tThen click on the List of all Prime Ministers of India\n",
    "#4.\tThen scrap the mentioned data and make theDataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c771090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d8cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('path_to_chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc944e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.jagranjosh.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d55d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_option = driver.find_element_by_link_text('GK')\n",
    "gk_option.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6359ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_option = driver.find_element_by_link_text('List of all Prime Ministers of India')\n",
    "pm_option.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "table = driver.find_element_by_xpath('//table[@class=\"table4\"]')\n",
    "rows = table.find_elements_by_tag_name('tr')\n",
    "for row in rows:\n",
    "  cols = row.find_elements_by_tag_name('td')\n",
    "  if len(cols) == 4:\n",
    "  name = cols[0].text\n",
    "  born_dead = cols[1].text\n",
    "  term_of_office = cols[2].text\n",
    "  remarks = cols[3].text\n",
    "  data.append([name, born_dead, term_of_office, remarks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104279bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['Name', 'Born-Dead', 'Term of Office', 'Remarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a47e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10: Write a python program to display list of 50 Most expensive cars in the world\n",
    "#(i.e. Car name and Price) from https://www.motor1.com/\n",
    "\n",
    "#This task will be done in following steps:\n",
    "#1.\tFirst get the webpage https://www.motor1.com/\n",
    "#2.\tThen You have to type in the search bar ’50 most expensive cars’\n",
    "#3.\tThen click on 50 most expensive cars in the world..\n",
    "#4.\tThen scrap the mentioned data and make the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53654830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f468e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the webpage\n",
    "driver = webdriver.Chrome('path_to_chromedriver')  # Replace 'path_to_chromedriver' with the actual path to your ChromeDriver executable\n",
    "driver.get('https://www.motor1.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Type in the search bar\n",
    "search_bar = driver.find_element_by_id('search-input')\n",
    "search_bar.send_keys('50 most expensive cars')\n",
    "search_bar.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Click on the link\n",
    "link = driver.find_element_by_link_text('50 Most Expensive Cars in the World')\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Scrape the data and create a dataframe\n",
    "car_names = driver.find_elements_by_xpath('//div[@class=\"article-content\"]/h3')\n",
    "car_prices = driver.find_elements_by_xpath('//div[@class=\"article-content\"]/p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10520eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for name, price in zip(car_names, car_prices):\n",
    "  data.append([name.text, price.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dfe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['Car Name', 'Price'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfa8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b873ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
